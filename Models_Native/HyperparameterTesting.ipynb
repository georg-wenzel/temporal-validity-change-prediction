{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "Run this block first to import all necessary libraries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import torch\n",
    "import itertools\n",
    "import pickle\n",
    "from transformers import BertModel, BertTokenizer, RobertaTokenizer, RobertaModel\n",
    "from utils.Datasets import TwitterDataset\n",
    "from utils.Functions import train_loop, collate_batch, eval_loop, set_up_deterministic_environment\n",
    "from utils.Models import TransformerClassifier, SiameseClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths, Variables and Setup\n",
    "Update paths to point to the correct files if necessary, update variables, and run the setup code blocks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_SET_PATH = '../data/dataset_splits/train.csv'\n",
    "VAL_SET_PATH = '../data/dataset_splits/val.csv'\n",
    "TEST_SET_PATH = '../data/dataset_splits/test.csv'\n",
    "HYPERPARAM_RESULTS_PATH = '../data/model_eval/hyperparameters/{}.pkl'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Variables\n",
    "# Set for the specific model that should be trained\n",
    "FREEZE_EMBEDDING_MODEL = True # True or False\n",
    "MODEL_NAME=\"roberta-base\" # \"roberta-base\" or \"bert-base-uncased\"\n",
    "MODEL_TYPE=\"siamese\" # \"siamese\" or \"transformer\"\n",
    "# Fixed variables\n",
    "RANDOM_SEED=42\n",
    "BATCH_SIZE=16\n",
    "MAX_EPOCHS=30\n",
    "# Hyperparameters to test\n",
    "LR=[1e-2, 1e-3, 1e-4]\n",
    "DROPOUT=[0.1, 0.25, 0.5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup: Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook Summary\n",
    "This notebook contains the training loop for hyperparameter testing. On execution of the code blocks in \"1. Hyperparameter Testing\", a model will be created based on the settings above (FREEZE_EMBEDDING_MODEL, MODEL_NAME, MODEL_TYPE). The model will be tested for MAX_EPOCHS for each combination of LR and DROPOUT. The model will stop training if no improvement to EM is observed for 5 epochs. The best-performing model (highest EM) will be evaluated on the test set. The results will be stored to a pickle file. When all EIGHT model combinations have been trained, the results can be evaluated by running the code blocks in \"2. Model Evaluation\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Hyperparameter Testing\n",
    "The code blocks below execute the hyperparameter testing loop for the defined MODEL_NAME, MODEL_TYPE and FREEZE_EMBEDDING_MODEL setting.\n",
    "We test the impact of dropout and learning rate on the exact match score of the model. The results are stored in a pickle file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Derive location of data from model name and param path\n",
    "model_str = MODEL_NAME.split('-')[0] + (\"_nofreeze\" if not FREEZE_EMBEDDING_MODEL else \"\") + (\"_siamese\" if MODEL_TYPE == \"siamese\" else \"\")\n",
    "HYPERPARAM_RESULTS_FILE = HYPERPARAM_RESULTS_PATH.format(model_str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "if MODEL_NAME.startswith(\"bert\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "elif MODEL_NAME.startswith(\"roberta\"):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model name: {MODEL_NAME}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Report: Epoch 17/30, Model roberta_siamese_dropout_0.5_lr_0.0001\n",
      "- Train loss: 0.6258\n",
      "- Val loss: 0.5364\n",
      "- Acc: 0.7837\n",
      "- EM: 0.4762\n",
      "Optimizing for EM, current best: 0.4881 (No improvement for 4 epochs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training roberta_siamese_dropout_0.5_lr_0.0001, Epoch 18/30: 100%|==========| 253/253 [00:08<00:00, 28.78it/s]\n",
      "Validating roberta_siamese_dropout_0.5_lr_0.0001, Epoch 18/30: 100%|==========| 32/32 [00:01<00:00, 31.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|==========| 32/32 [00:00<00:00, 32.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5353815713897347\n",
      "Accuracy: 0.8015873015873016\n",
      "EM: 0.5178571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_up_deterministic_environment(RANDOM_SEED)\n",
    "settings = itertools.product(LR, DROPOUT)\n",
    "results = []\n",
    "\n",
    "for i, (lr, dropout) in enumerate(settings, 1):\n",
    "    print(f'Now training model ({model_str}): {dropout} dropout, {lr} starting LR. (Model {i}/{len(LR)*len(DROPOUT)})')\n",
    "\n",
    "    # Get model\n",
    "    if MODEL_NAME.startswith(\"bert\"):\n",
    "        model = BertModel.from_pretrained(MODEL_NAME)\n",
    "    elif MODEL_NAME.startswith(\"roberta\"):\n",
    "        model = RobertaModel.from_pretrained(MODEL_NAME)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {MODEL_NAME}\")\n",
    "\n",
    "    ds_train = TwitterDataset(TRAIN_SET_PATH, tokenizer, split_input=(MODEL_TYPE == \"siamese\"))\n",
    "    ds_val = TwitterDataset(VAL_SET_PATH, tokenizer, split_input=(MODEL_TYPE == \"siamese\"))\n",
    "    ds_test = TwitterDataset(TEST_SET_PATH, tokenizer, split_input=(MODEL_TYPE == \"siamese\"))\n",
    "    if MODEL_TYPE == \"transformer\":\n",
    "        cls = TransformerClassifier(model, dropout=dropout, freeze_embedding_model=FREEZE_EMBEDDING_MODEL).to(device)\n",
    "    elif MODEL_TYPE == \"siamese\":\n",
    "        cls = SiameseClassifier(model, freeze_embedding_model=FREEZE_EMBEDDING_MODEL, dropout=dropout).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {MODEL_TYPE}\")\n",
    "\n",
    "    best_model, train_losses, val_losses, accs, ems = train_loop(cls, ds_train, ds_val, partial(collate_batch, input_padding=(1 if MODEL_NAME.startswith(\"roberta\") else 0)), device, batch_size=BATCH_SIZE, max_epochs=MAX_EPOCHS, lr=lr, patience=5, name=f'{model_str}_dropout_{dropout}_lr_{lr}')\n",
    "    test_loss, acc, em = eval_loop(best_model, ds_test, partial(collate_batch, input_padding=(1 if MODEL_NAME.startswith(\"roberta\") else 0)), device, batch_size=BATCH_SIZE)\n",
    "    results.append((lr, dropout, train_losses, val_losses, accs, ems, test_loss, acc, em))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Store results\n",
    "with open(HYPERPARAM_RESULTS_FILE, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Model Evaluation\n",
    "The model evaluation assumes that files exist in the HYPERPARAM_RESULTS_PATH for all combinations of MODEL_NAME, MODEL_TYPE and FREEZE_EMBEDDING_MODEL. The code block below loads the results into separate dataframes for the transformer and Siamese models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Function to specifically deal with hyperparameter evaluation results\n",
    "def load_hyperparam_eval(eval_path: str, models: List[str]) -> pd.DataFrame:\n",
    "    best_model_info = []\n",
    "\n",
    "    for model in models:\n",
    "        path = eval_path.format(model)\n",
    "        with open(path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "\n",
    "        for classifier in results:\n",
    "            lr, dropout, train_losses, val_losses, accs, ems, test_loss, acc, em = classifier\n",
    "            best_epoch = ems.index(max(ems)) + 1\n",
    "            best_model_info.append((model, lr, dropout, best_epoch, test_loss, acc, em))\n",
    "\n",
    "    df = pd.DataFrame(best_model_info, columns=[\"model\", \"lr\", \"dropout\", \"best_epoch\", \"test_loss\", \"acc\", \"em\"])\n",
    "    df[[\"test_loss\", \"acc\", \"em\"]] = df[[\"test_loss\", \"acc\", \"em\"]].round(3)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "               model      lr  dropout  best_epoch  test_loss    acc     em\n25     bert_nofreeze  0.0001     0.25           5      0.596  0.853  0.613\n24     bert_nofreeze  0.0001     0.10           6      0.647  0.827  0.548\n26     bert_nofreeze  0.0001     0.50           4      0.450  0.839  0.548\n7               bert  0.0001     0.25          17      0.639  0.726  0.321\n6               bert  0.0001     0.10           8      0.687  0.720  0.315\n3               bert  0.0010     0.10          10      0.683  0.718  0.304\n5               bert  0.0010     0.50          15      0.695  0.712  0.286\n8               bert  0.0001     0.50          20      0.661  0.710  0.280\n4               bert  0.0010     0.25           8      0.706  0.712  0.262\n13           roberta  0.0010     0.25          14      0.761  0.677  0.262\n15           roberta  0.0001     0.10          16      0.805  0.667  0.256\n14           roberta  0.0010     0.50          15      0.733  0.665  0.238\n12           roberta  0.0010     0.10          10      0.790  0.673  0.238\n16           roberta  0.0001     0.25          13      0.819  0.637  0.173\n17           roberta  0.0001     0.50           9      0.915  0.595  0.161\n31  roberta_nofreeze  0.0010     0.25           1      1.103  0.333  0.000\n32  roberta_nofreeze  0.0010     0.50           1      1.104  0.333  0.000\n33  roberta_nofreeze  0.0001     0.10           1      1.100  0.333  0.000\n30  roberta_nofreeze  0.0010     0.10           1      1.099  0.333  0.000\n29  roberta_nofreeze  0.0100     0.50           1      1.099  0.333  0.000\n28  roberta_nofreeze  0.0100     0.25           1      1.099  0.333  0.000\n27  roberta_nofreeze  0.0100     0.10           1      1.099  0.333  0.000\n34  roberta_nofreeze  0.0001     0.25           1      1.101  0.333  0.000\n0               bert  0.0100     0.10           1      1.099  0.333  0.000\n18     bert_nofreeze  0.0100     0.10           1      1.099  0.333  0.000\n23     bert_nofreeze  0.0010     0.50           1      1.099  0.333  0.000\n22     bert_nofreeze  0.0010     0.25           1      1.100  0.333  0.000\n21     bert_nofreeze  0.0010     0.10           1      1.180  0.333  0.000\n20     bert_nofreeze  0.0100     0.50           1      1.099  0.333  0.000\n19     bert_nofreeze  0.0100     0.25           1      1.099  0.333  0.000\n1               bert  0.0100     0.25           1      1.099  0.333  0.000\n11           roberta  0.0100     0.50           1      1.099  0.333  0.000\n10           roberta  0.0100     0.25           1      1.099  0.333  0.000\n9            roberta  0.0100     0.10           1      1.099  0.333  0.000\n2               bert  0.0100     0.50           1      1.099  0.333  0.000\n35  roberta_nofreeze  0.0001     0.50           1      1.108  0.333  0.000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>lr</th>\n      <th>dropout</th>\n      <th>best_epoch</th>\n      <th>test_loss</th>\n      <th>acc</th>\n      <th>em</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25</th>\n      <td>bert_nofreeze</td>\n      <td>0.0001</td>\n      <td>0.25</td>\n      <td>5</td>\n      <td>0.596</td>\n      <td>0.853</td>\n      <td>0.613</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bert_nofreeze</td>\n      <td>0.0001</td>\n      <td>0.10</td>\n      <td>6</td>\n      <td>0.647</td>\n      <td>0.827</td>\n      <td>0.548</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>bert_nofreeze</td>\n      <td>0.0001</td>\n      <td>0.50</td>\n      <td>4</td>\n      <td>0.450</td>\n      <td>0.839</td>\n      <td>0.548</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>bert</td>\n      <td>0.0001</td>\n      <td>0.25</td>\n      <td>17</td>\n      <td>0.639</td>\n      <td>0.726</td>\n      <td>0.321</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bert</td>\n      <td>0.0001</td>\n      <td>0.10</td>\n      <td>8</td>\n      <td>0.687</td>\n      <td>0.720</td>\n      <td>0.315</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert</td>\n      <td>0.0010</td>\n      <td>0.10</td>\n      <td>10</td>\n      <td>0.683</td>\n      <td>0.718</td>\n      <td>0.304</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>bert</td>\n      <td>0.0010</td>\n      <td>0.50</td>\n      <td>15</td>\n      <td>0.695</td>\n      <td>0.712</td>\n      <td>0.286</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>bert</td>\n      <td>0.0001</td>\n      <td>0.50</td>\n      <td>20</td>\n      <td>0.661</td>\n      <td>0.710</td>\n      <td>0.280</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert</td>\n      <td>0.0010</td>\n      <td>0.25</td>\n      <td>8</td>\n      <td>0.706</td>\n      <td>0.712</td>\n      <td>0.262</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>roberta</td>\n      <td>0.0010</td>\n      <td>0.25</td>\n      <td>14</td>\n      <td>0.761</td>\n      <td>0.677</td>\n      <td>0.262</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>roberta</td>\n      <td>0.0001</td>\n      <td>0.10</td>\n      <td>16</td>\n      <td>0.805</td>\n      <td>0.667</td>\n      <td>0.256</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>roberta</td>\n      <td>0.0010</td>\n      <td>0.50</td>\n      <td>15</td>\n      <td>0.733</td>\n      <td>0.665</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>roberta</td>\n      <td>0.0010</td>\n      <td>0.10</td>\n      <td>10</td>\n      <td>0.790</td>\n      <td>0.673</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>roberta</td>\n      <td>0.0001</td>\n      <td>0.25</td>\n      <td>13</td>\n      <td>0.819</td>\n      <td>0.637</td>\n      <td>0.173</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>roberta</td>\n      <td>0.0001</td>\n      <td>0.50</td>\n      <td>9</td>\n      <td>0.915</td>\n      <td>0.595</td>\n      <td>0.161</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0010</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.103</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0010</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.104</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0001</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.100</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0010</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0100</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0100</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0001</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.101</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>bert</td>\n      <td>0.0100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>bert_nofreeze</td>\n      <td>0.0100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bert_nofreeze</td>\n      <td>0.0010</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>bert_nofreeze</td>\n      <td>0.0010</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.100</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>bert_nofreeze</td>\n      <td>0.0010</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.180</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>bert_nofreeze</td>\n      <td>0.0100</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>bert_nofreeze</td>\n      <td>0.0100</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert</td>\n      <td>0.0100</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>roberta</td>\n      <td>0.0100</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>roberta</td>\n      <td>0.0100</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>roberta</td>\n      <td>0.0100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert</td>\n      <td>0.0100</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>roberta_nofreeze</td>\n      <td>0.0001</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.108</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_hyperparam_eval(HYPERPARAM_RESULTS_PATH, [\"bert\", \"roberta\", \"bert_nofreeze\", \"roberta_nofreeze\"]).sort_values(\"em\", ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                       model      lr  dropout  best_epoch  test_loss    acc  \\\n25     bert_nofreeze_siamese  0.0001     0.25           7      0.637  0.847   \n24     bert_nofreeze_siamese  0.0001     0.10           4      0.481  0.839   \n26     bert_nofreeze_siamese  0.0001     0.50           2      0.513  0.837   \n15           roberta_siamese  0.0001     0.10          21      0.503  0.817   \n17           roberta_siamese  0.0001     0.50          13      0.535  0.802   \n16           roberta_siamese  0.0001     0.25          17      0.507  0.804   \n12           roberta_siamese  0.0010     0.10          10      0.550  0.800   \n14           roberta_siamese  0.0010     0.50           8      0.538  0.784   \n13           roberta_siamese  0.0010     0.25           8      0.567  0.780   \n8               bert_siamese  0.0001     0.50           9      0.606  0.748   \n4               bert_siamese  0.0010     0.25           8      0.639  0.722   \n7               bert_siamese  0.0001     0.25           5      0.620  0.714   \n6               bert_siamese  0.0001     0.10           6      0.638  0.726   \n3               bert_siamese  0.0010     0.10           7      0.654  0.714   \n5               bert_siamese  0.0010     0.50           7      0.649  0.726   \n0               bert_siamese  0.0100     0.10           3      0.829  0.563   \n31  roberta_nofreeze_siamese  0.0010     0.25           1      1.099  0.333   \n32  roberta_nofreeze_siamese  0.0010     0.50           1      1.104  0.333   \n33  roberta_nofreeze_siamese  0.0001     0.10           1      1.099  0.333   \n30  roberta_nofreeze_siamese  0.0010     0.10           1      1.099  0.333   \n29  roberta_nofreeze_siamese  0.0100     0.50           1      1.099  0.333   \n28  roberta_nofreeze_siamese  0.0100     0.25           1      1.099  0.333   \n27  roberta_nofreeze_siamese  0.0100     0.10           1      1.099  0.333   \n34  roberta_nofreeze_siamese  0.0001     0.25           1      1.100  0.333   \n18     bert_nofreeze_siamese  0.0100     0.10           1      1.100  0.333   \n23     bert_nofreeze_siamese  0.0010     0.50           1      1.099  0.333   \n22     bert_nofreeze_siamese  0.0010     0.25           1      1.099  0.333   \n21     bert_nofreeze_siamese  0.0010     0.10           1      1.099  0.333   \n20     bert_nofreeze_siamese  0.0100     0.50           1      1.099  0.333   \n19     bert_nofreeze_siamese  0.0100     0.25           1      1.099  0.333   \n1               bert_siamese  0.0100     0.25           1      1.099  0.333   \n11           roberta_siamese  0.0100     0.50           1      1.099  0.333   \n10           roberta_siamese  0.0100     0.25           1      1.099  0.333   \n9            roberta_siamese  0.0100     0.10           1      1.099  0.333   \n2               bert_siamese  0.0100     0.50           1      1.099  0.333   \n35  roberta_nofreeze_siamese  0.0001     0.50           1      1.099  0.333   \n\n       em  \n25  0.589  \n24  0.577  \n26  0.565  \n15  0.548  \n17  0.518  \n16  0.512  \n12  0.506  \n14  0.464  \n13  0.446  \n8   0.387  \n4   0.357  \n7   0.339  \n6   0.339  \n3   0.333  \n5   0.333  \n0   0.030  \n31  0.000  \n32  0.000  \n33  0.000  \n30  0.000  \n29  0.000  \n28  0.000  \n27  0.000  \n34  0.000  \n18  0.000  \n23  0.000  \n22  0.000  \n21  0.000  \n20  0.000  \n19  0.000  \n1   0.000  \n11  0.000  \n10  0.000  \n9   0.000  \n2   0.000  \n35  0.000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>lr</th>\n      <th>dropout</th>\n      <th>best_epoch</th>\n      <th>test_loss</th>\n      <th>acc</th>\n      <th>em</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0001</td>\n      <td>0.25</td>\n      <td>7</td>\n      <td>0.637</td>\n      <td>0.847</td>\n      <td>0.589</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0001</td>\n      <td>0.10</td>\n      <td>4</td>\n      <td>0.481</td>\n      <td>0.839</td>\n      <td>0.577</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0001</td>\n      <td>0.50</td>\n      <td>2</td>\n      <td>0.513</td>\n      <td>0.837</td>\n      <td>0.565</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>roberta_siamese</td>\n      <td>0.0001</td>\n      <td>0.10</td>\n      <td>21</td>\n      <td>0.503</td>\n      <td>0.817</td>\n      <td>0.548</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>roberta_siamese</td>\n      <td>0.0001</td>\n      <td>0.50</td>\n      <td>13</td>\n      <td>0.535</td>\n      <td>0.802</td>\n      <td>0.518</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>roberta_siamese</td>\n      <td>0.0001</td>\n      <td>0.25</td>\n      <td>17</td>\n      <td>0.507</td>\n      <td>0.804</td>\n      <td>0.512</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>roberta_siamese</td>\n      <td>0.0010</td>\n      <td>0.10</td>\n      <td>10</td>\n      <td>0.550</td>\n      <td>0.800</td>\n      <td>0.506</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>roberta_siamese</td>\n      <td>0.0010</td>\n      <td>0.50</td>\n      <td>8</td>\n      <td>0.538</td>\n      <td>0.784</td>\n      <td>0.464</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>roberta_siamese</td>\n      <td>0.0010</td>\n      <td>0.25</td>\n      <td>8</td>\n      <td>0.567</td>\n      <td>0.780</td>\n      <td>0.446</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>bert_siamese</td>\n      <td>0.0001</td>\n      <td>0.50</td>\n      <td>9</td>\n      <td>0.606</td>\n      <td>0.748</td>\n      <td>0.387</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert_siamese</td>\n      <td>0.0010</td>\n      <td>0.25</td>\n      <td>8</td>\n      <td>0.639</td>\n      <td>0.722</td>\n      <td>0.357</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>bert_siamese</td>\n      <td>0.0001</td>\n      <td>0.25</td>\n      <td>5</td>\n      <td>0.620</td>\n      <td>0.714</td>\n      <td>0.339</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bert_siamese</td>\n      <td>0.0001</td>\n      <td>0.10</td>\n      <td>6</td>\n      <td>0.638</td>\n      <td>0.726</td>\n      <td>0.339</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert_siamese</td>\n      <td>0.0010</td>\n      <td>0.10</td>\n      <td>7</td>\n      <td>0.654</td>\n      <td>0.714</td>\n      <td>0.333</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>bert_siamese</td>\n      <td>0.0010</td>\n      <td>0.50</td>\n      <td>7</td>\n      <td>0.649</td>\n      <td>0.726</td>\n      <td>0.333</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>bert_siamese</td>\n      <td>0.0100</td>\n      <td>0.10</td>\n      <td>3</td>\n      <td>0.829</td>\n      <td>0.563</td>\n      <td>0.030</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0010</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0010</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.104</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0001</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0010</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0100</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0100</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0001</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.100</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.100</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0010</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0010</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0010</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0100</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>bert_nofreeze_siamese</td>\n      <td>0.0100</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert_siamese</td>\n      <td>0.0100</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>roberta_siamese</td>\n      <td>0.0100</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>roberta_siamese</td>\n      <td>0.0100</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>roberta_siamese</td>\n      <td>0.0100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert_siamese</td>\n      <td>0.0100</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>roberta_nofreeze_siamese</td>\n      <td>0.0001</td>\n      <td>0.50</td>\n      <td>1</td>\n      <td>1.099</td>\n      <td>0.333</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_hyperparam_eval(HYPERPARAM_RESULTS_PATH, [\"bert_siamese\", \"roberta_siamese\", \"bert_nofreeze_siamese\", \"roberta_nofreeze_siamese\"]).sort_values(\"em\", ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
