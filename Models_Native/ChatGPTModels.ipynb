{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "Run this block first to import all necessary libraries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "from IPython.core.display_functions import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths, Variables and Setup\n",
    "Update paths to point to the correct files if necessary, update variables, and run the setup code blocks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR=\"../data/dataset_splits/holdout_training/test{}.csv\"\n",
    "OUTPUT_DIR=\"../data/model_eval/chatgpt/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variables\n",
    "API_KEY_ENV_NAME=\"OPENAI_API_KEY\"\n",
    "class_mappings = {\n",
    "    \"Decreased\": 0,\n",
    "    \"Neutral\": 1,\n",
    "    \"Increased\": 2\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup: Environment\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Setup: Function that appends a dataset instance to the predefined prompt.\n",
    "def append_prompt(prompt, target, follow_up):\n",
    "    return prompt + [{\n",
    "        \"role\": \"user\", \"content\":\n",
    "        f\"\"\"Sentence A: \"{target}\"\n",
    "Sentence B: \"{follow_up}\\\"\"\"\"\n",
    "    }]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup: Function that extracts the response from the API call into a structured form.\n",
    "def extract_response(target, follow_up, completion):\n",
    "    completion_text = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    cls_txt = completion_text.split(\"\\n\")[-1].strip()\n",
    "    return {\n",
    "        \"target\": target,\n",
    "        \"follow_up\": follow_up,\n",
    "        \"explanation\": completion_text.split(\"```\")[1].strip(),\n",
    "        \"class\": class_mappings[cls_txt] if cls_txt in class_mappings else -1\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook Summary\n",
    "This notebook contains the logic for obtaining dataset results using OpenAI's API. We define a few-shot chain-of-thought reasoning prompt and evaluate all dataset instances over the OpenAI API. The total cost for GPT3.5 was around €5. We evaluate the model's predictions and calculate accuracy and exact match metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Define Prompt\n",
    "In our prompt, we provide a task as well as some few-shot examples to the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "message_prompt = [\n",
    "  {\"role\": \"system\", \"content\":\n",
    "    \"\"\"You are a language model specializing in temporal commonsense reasoning. Each prompt contains Sentence A and Sentence B. You should determine whether Sentence B changes the expected temporal validity duration of Sentence A, i.e., the duration for which the information in Sentence A is expected to be relevant to a reader.\n",
    "\n",
    "To achieve this, in your responses, first, estimate for how long the average reader may expect Sentence A to be relevant on its own. Then, consider if the information introduced in Sentence B increases or decreases this duration. Surround this explanation in triple backticks (```).\n",
    "\n",
    "After your explanation, respond with one of the three possible classes corresponding to your explanation: \"Decreased\", \"Neutral\", or \"Increased\".\"\"\"\n",
    "   },\n",
    "  {\n",
    "    \"role\": \"user\", \"content\":\n",
    "    \"\"\"Sentence A: \"i'm ready to go to the beach.\"\n",
    "Sentence B: \"I forgot all the beach towels are still in the dryer, but I'll be ready to go as soon as the dryer's done running.\\\"\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\", \"content\":\n",
    "    \"\"\"```Going to the beach may take a few minutes to an hour, depending on the distance. However, if the author first needs to wait on the dryer to finish in order to retrieve their beach towels, this may take an additional 30-60 minutes.```\n",
    "\n",
    "Increased\"\"\"\n",
    "  },\n",
    "  {\n",
    "      \"role\": \"user\", \"content\":\n",
    "      \"\"\"\"Sentence A: taking bad thoughts out of my mind thru grinding my assignments\"\n",
    "Sentence B: \"I just have to get through a short math homework assignment and memorize a few spelling words so it shouldn't take long.\\\"\"\"\"\n",
    "  },\n",
    "  {\n",
    "      \"role\": \"assistant\", \"content\":\n",
    "      \"\"\"```Grinding through assignments may take several hours, depending on the number of assignments to complete. In Sentence B, the author states they only have a few short assignments remaining, so they may only take an hour or less to finish them.```\n",
    "\n",
    "Decreased\"\"\"\n",
    "  },\n",
    "  {\n",
    "        \"role\": \"user\", \"content\":\n",
    "        \"\"\"Sentence A: \"Slide to my dm guys, come on\"\n",
    "Sentence B: \"Instagram DMs are such a fun way to communicate.\\\"\"\"\"\n",
    "  },\n",
    "  {\n",
    "        \"role\": \"assistant\", \"content\":\n",
    "        \"\"\"```The author encourages people to direct message them, which may be relevant for several minutes to a few hours. Sentence B does not change the duration for which Sentence A is expected to be relevant.```\n",
    "\n",
    "Neutral\"\"\"\n",
    "  }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Setup OpenAI Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(API_KEY_ENV_NAME)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"```The need for praying for the exam is relevant until the exam is completed, which is typically a few hours. However, in Sentence B, it is mentioned that the exams will last all week, indicating that the need for praying for exams will exist beyond the immediate exam for tomorrow. Therefore, the duration for which Sentence A is expected to be relevant is increased.```\\n\\nIncreased\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1690802277,\n",
      "  \"id\": \"chatcmpl-7iLCP7b1mAjRIShgv328VWUnxXGIP\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 76,\n",
      "    \"prompt_tokens\": 474,\n",
      "    \"total_tokens\": 550\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# We can prompt the model as follows (using target and follow-up statement)\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=append_prompt(message_prompt, \"we all need to pray for exam for tomorrow\", \"And tomorrow's just the first of the exams that last all week.\")\n",
    ")\n",
    "\n",
    "print(completion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'target': 'we all need to pray for exam for tomorrow',\n 'follow_up': \"And tomorrow's just the first of the exams that last all week.\",\n 'explanation': 'The need for praying for the exam is relevant until the exam is completed, which is typically a few hours. However, in Sentence B, it is mentioned that the exams will last all week, indicating that the need for praying for exams will exist beyond the immediate exam for tomorrow. Therefore, the duration for which Sentence A is expected to be relevant is increased.',\n 'class': 2}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We bring the response into a structured form using the extract_response function\n",
    "extract_response(\"we all need to pray for exam for tomorrow\", \"And tomorrow's just the first of the exams that last all week.\", completion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Generate Predictions\n",
    "To test the model, we provide one of the folds of the dataset as input to the model and evaluate the model's predictions. Since these calls incur costs and are non-deterministic, we store the results in a file. The few-shot examples provided to the prompt are from cv2. This means the model has seen 3 examples from the given fold in its prompt, which may have a very minor positive impact on performance of that fold."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def evaluate_fold(input_df, fold_num, output_df=None, from_idx=0):\n",
    "    if output_df is None:\n",
    "        results = []\n",
    "\n",
    "    else:\n",
    "        results = output_df.to_dict(\"records\")\n",
    "\n",
    "    for i, row in tqdm(input_df.iterrows(), total=len(input_df), desc=f\"Evaluating Fold {fold_num}\"):\n",
    "        # skip if we already have a result for this row\n",
    "        if i < from_idx:\n",
    "            continue\n",
    "        if i % 10 == 0:\n",
    "            clear_output()\n",
    "        while True:\n",
    "            try:\n",
    "                target = row[\"context\"]\n",
    "                follow_up = row[\"follow_up\"]\n",
    "                completion = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=append_prompt(message_prompt, target, follow_up)\n",
    "                )\n",
    "                results.append(extract_response(target, follow_up, completion))\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Error occurred. Retrying after 1 minute...\")\n",
    "                print(e)\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "\n",
    "    # save results to file\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_DIR + f\"cv{fold_num}_results.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Fold 4: 100%|██████████| 1011/1011 [1:14:26<00:00,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# iterate over folds\n",
    "for i in range(5):\n",
    "    input_df = pd.read_csv(DATA_DIR.format(i))\n",
    "    # check if output file exists\n",
    "    if os.path.exists(OUTPUT_DIR + f\"cv{i}_results.csv\"):\n",
    "        # If so, check if there are still missing items\n",
    "        output_df = pd.read_csv(OUTPUT_DIR + f\"cv{i}_results.csv\")\n",
    "        if len(output_df) < len(input_df):\n",
    "            # If so, we need to evaluate the missing items\n",
    "            print(f\"Evaluating fold {i} from index {len(output_df)}.\")\n",
    "            evaluate_fold(input_df, i, output_df, from_idx=len(output_df))\n",
    "        else:\n",
    "            # Otherwise, we can skip this fold\n",
    "            print(f\"Skipping fold {i} because all items have already been evaluated.\")\n",
    "            continue\n",
    "    else:\n",
    "        # If not, we need to evaluate the entire fold\n",
    "        print(f\"Evaluating fold {i} from index 0.\")\n",
    "        evaluate_fold(input_df, i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Evaluate Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_acc_em(eval_df):\n",
    "    acc = len(eval_df[eval_df[\"class\"] == eval_df[\"predicted_class\"]]) / len(eval_df)\n",
    "\n",
    "    target_tweet_map = {}\n",
    "\n",
    "    for i, row in eval_df.iterrows():\n",
    "        # Add tweet to map if not exists\n",
    "        if row[\"target\"] not in target_tweet_map:\n",
    "            target_tweet_map[row[\"target\"]] = 0\n",
    "        # Check if prediction matches the label, increase counter\n",
    "        if row[\"class\"] == row[\"predicted_class\"]:\n",
    "            target_tweet_map[row[\"target\"]] += 1\n",
    "\n",
    "    em = len([t for t in target_tweet_map.keys() if target_tweet_map[t] == 3]) / len(target_tweet_map)\n",
    "\n",
    "    return acc, em"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "results = None\n",
    "\n",
    "for i in range(5):\n",
    "    result_df = pd.read_csv(OUTPUT_DIR + f\"cv{i}_results.csv\").rename(columns={\"class\": \"predicted_class\"})\n",
    "    fold_df = pd.read_csv(DATA_DIR.format(i)).rename(columns={\"context\": \"target\"})\n",
    "    fold_df[\"class\"] = fold_df[\"change\"].map({\"decreased\": 0, \"neutral\": 1, \"increased\": 2})\n",
    "    eval_df = pd.DataFrame(result_df).merge(fold_df, how=\"left\")\n",
    "\n",
    "    acc, em = get_acc_em(eval_df)\n",
    "\n",
    "    fold_results = pd.DataFrame([{\n",
    "        \"fold\": f\"chatgpt_{i}\",\n",
    "        \"accuracy\": acc,\n",
    "        \"exact_match\": em\n",
    "    }])\n",
    "\n",
    "    if i == 0:\n",
    "        results = fold_results\n",
    "    else:\n",
    "        results = pd.concat((results,fold_results))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "        fold  accuracy  exact_match\n0  chatgpt_0  0.611276     0.246291\n0  chatgpt_1  0.693373     0.326409\n0  chatgpt_2  0.638971     0.252226\n0  chatgpt_3  0.683482     0.314540\n0  chatgpt_4  0.687438     0.323442",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold</th>\n      <th>accuracy</th>\n      <th>exact_match</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>chatgpt_0</td>\n      <td>0.611276</td>\n      <td>0.246291</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>chatgpt_1</td>\n      <td>0.693373</td>\n      <td>0.326409</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>chatgpt_2</td>\n      <td>0.638971</td>\n      <td>0.252226</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>chatgpt_3</td>\n      <td>0.683482</td>\n      <td>0.314540</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>chatgpt_4</td>\n      <td>0.687438</td>\n      <td>0.323442</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.663\n",
      "Mean Exact Match: 0.293\n",
      "Accuracy Std.: 0.036\n",
      "Exact Match Std.: 0.04\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Accuracy: {results['accuracy'].mean().round(3)}\")\n",
    "print(f\"Mean Exact Match: {results['exact_match'].mean().round(3)}\")\n",
    "print(f\"Accuracy Std.: {round(results['accuracy'].std(), 3)}\")\n",
    "print(f\"Exact Match Std.: {round(results['exact_match'].std(), 3)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
